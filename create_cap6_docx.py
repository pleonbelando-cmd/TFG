"""Genera Capitulo 06 ML.docx con el mismo formato que los capÃ­tulos anteriores."""

import os
from docx import Document
from docx.shared import Pt, RGBColor, Cm, Emu, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn
import copy

FIGURES_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "output", "figures")

# â”€â”€ Colores â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
H1_COLOR = RGBColor(0x36, 0x5F, 0x91)
H2_COLOR = RGBColor(0x4F, 0x81, 0xBD)
H3_COLOR = RGBColor(0x4F, 0x81, 0xBD)

def add_figure(doc, filename, caption, width_inches=5.5):
    """Inserta una imagen centrada con pie de figura."""
    img_path = os.path.join(FIGURES_DIR, filename)
    if not os.path.exists(img_path):
        # Fallback: aviso en el documento en lugar de romper el script
        p = doc.add_paragraph(f"[Imagen no encontrada: {filename}]")
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        p.runs[0].italic = True
    else:
        p = doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.CENTER
        run = p.add_run()
        run.add_picture(img_path, width=Inches(width_inches))
    # Pie de figura en cursiva y centrado
    cap_p = doc.add_paragraph(caption)
    cap_p.alignment = WD_ALIGN_PARAGRAPH.CENTER
    cap_p.paragraph_format.space_after = Pt(8)
    for run in cap_p.runs:
        run.italic = True
        run.font.size = Pt(9)
    return cap_p


def make_doc():
    doc = Document()

    # Page setup
    for section in doc.sections:
        section.page_width  = Cm(21.6)
        section.page_height = Cm(27.9)
        section.top_margin    = Cm(2.5)
        section.bottom_margin = Cm(2.5)
        section.left_margin   = Cm(3.0)
        section.right_margin  = Cm(3.0)

    # Apply styles to match existing chapters
    for sty_name, size_emu, color in [
        ("Heading 1", 177800, H1_COLOR),
        ("Heading 2", 165100, H2_COLOR),
        ("Heading 3", 152400, H3_COLOR),
    ]:
        sty = doc.styles[sty_name]
        sty.font.size  = Emu(size_emu)
        sty.font.bold  = True
        sty.font.color.rgb = color

    normal = doc.styles["Normal"]
    normal.font.size = Pt(11)
    normal.font.name = "Calibri"

    return doc


def h1(doc, text):
    p = doc.add_heading(text, level=1)
    return p

def h2(doc, text):
    p = doc.add_heading(text, level=2)
    return p

def h3(doc, text):
    p = doc.add_heading(text, level=3)
    return p

def para(doc, text, bold=False):
    p = doc.add_paragraph(text)
    if bold:
        for run in p.runs:
            run.bold = True
    return p

def formula(doc, text):
    """PÃ¡rrafo centrado para ecuaciones."""
    p = doc.add_paragraph(text)
    p.alignment = WD_ALIGN_PARAGRAPH.CENTER
    p.paragraph_format.space_before = Pt(4)
    p.paragraph_format.space_after  = Pt(4)
    return p

def add_table(doc, headers, rows, caption=None):
    """Tabla con cabecera."""
    if caption:
        cap_p = doc.add_paragraph(caption)
        cap_p.runs[0].bold = True
        cap_p.runs[0].italic = True

    t = doc.add_table(rows=1 + len(rows), cols=len(headers))
    t.style = "Table Grid"

    # Header row
    hdr = t.rows[0].cells
    for i, h in enumerate(headers):
        hdr[i].text = h
        hdr[i].paragraphs[0].runs[0].bold = True

    # Data rows
    for ri, row in enumerate(rows):
        cells = t.rows[ri + 1].cells
        for ci, val in enumerate(row):
            cells[ci].text = str(val)

    doc.add_paragraph()  # space after table


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
doc = make_doc()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
h1(doc, "CapÃ­tulo 6: Modelos de Machine Learning para la PredicciÃ³n del Retorno del Oro")

# 6.1
h2(doc, "6.1 MotivaciÃ³n y enfoque")
para(doc, "Los modelos economÃ©tricos del CapÃ­tulo 5 permitieron identificar las relaciones estructurales entre el precio del oro y sus determinantes macroeconÃ³micos: la existencia de un Ãºnico vector de cointegraciÃ³n (r = 1 segÃºn el criterio max-eigenvalue), la causalidad Granger unidireccional desde los tipos de interÃ©s reales, y la asimetrÃ­a de la volatilidad condicional capturada por el GJR-GARCH. Sin embargo, los modelos lineales presentan limitaciones intrÃ­nsecas cuando los fenÃ³menos de interÃ©s son no lineales, cuando las interacciones entre variables resultan complejas, o cuando la dinÃ¡mica del sistema cambia de rÃ©gimen.")
para(doc, "El machine learning ofrece una perspectiva complementaria: en lugar de partir de relaciones teÃ³ricas a priori, los algoritmos aprenden patrones directamente de los datos, capturando interacciones de alto orden y no linealidades sin especificaciÃ³n manual. La literatura reciente (Plakandaras et al., 2022; Liang et al., 2023) documenta que los modelos de conjuntos y las redes recurrentes mejoran la predicciÃ³n de corto plazo del oro en horizontes de uno a tres meses, especialmente durante episodios de alta incertidumbre.")
para(doc, "Este capÃ­tulo implementa tres arquitecturas complementarias: XGBoost (gradient boosting sobre Ã¡rboles de decisiÃ³n), Random Forest (conjunto de Ã¡rboles independientes), y LSTM (red neuronal recurrente con memoria de largo-corto plazo). La evaluaciÃ³n sigue el protocolo walk-forward con ventana expandible, que elimina el sesgo de look-ahead y reproduce fielmente la situaciÃ³n de un inversor que opera en tiempo real.")

# 6.2
h2(doc, "6.2 IngenierÃ­a de caracterÃ­sticas")

h3(doc, "6.2.1 Variables base y transformaciones")
para(doc, "La matriz de caracterÃ­sticas se construye a partir del conjunto de datos maestro (312 observaciones mensuales, enero 2000 â€” diciembre 2025), con transformaciones para garantizar la estacionariedad de todas las entradas:")
para(doc, "â€¢ Retornos logarÃ­tmicos: DXY e Ã­ndice WTI se transforman mediante r_t = ln(P_t / P_{t-1}) para convertir precios no estacionarios en series estacionarias.")
para(doc, "â€¢ Primera diferencia: el VIX se diferencia (Î”VIX_t) para capturar el cambio en el rÃ©gimen de volatilidad en lugar del nivel.")
para(doc, "â€¢ Niveles: TIPS 10 aÃ±os, Fed Funds, CPI interanual y Breakeven se mantienen en niveles, dado que son tasas ya estacionarias (I(0) segÃºn los tests del CapÃ­tulo 4).")

h3(doc, "6.2.2 Retardos temporales")
para(doc, "Para cada variable base se generan retardos de orden 1, 2 y 3 (correspondientes a 1, 2 y 3 meses de historia). Este diseÃ±o evita cualquier filtraciÃ³n de informaciÃ³n futura: en el momento en que el modelo realiza la predicciÃ³n para el mes t, solo dispone de observaciones hasta t-1 (retardo 1) o anteriores.")

h3(doc, "6.2.3 Indicadores de inercia del oro")
para(doc, "Se incluyen tres indicadores de momentum calculados sobre el precio del propio oro: MA3_oro (media mÃ³vil de los retornos de los Ãºltimos 3 meses, desplazada un perÃ­odo), MA6_oro (anÃ¡logo con ventana de 6 meses), y Vol3_oro (desviaciÃ³n estÃ¡ndar de los retornos de los Ãºltimos 3 meses, proxy de volatilidad realizada reciente).")

h3(doc, "6.2.4 Variable de rÃ©gimen")
para(doc, "Se incorpora una dummy binaria (is_crisis) que vale 1 en los cinco episodios histÃ³ricos identificados en el CapÃ­tulo 2 (GFC, Post-QE, COVID-19, ciclo de subidas de tipos, confluencia 2025) y 0 en perÃ­odos de calma. Esta variable permite a los modelos identificar si el entorno actual es de crisis.")

h3(doc, "6.2.5 Matriz final")
para(doc, "Tras aplicar los retardos y ventanas deslizantes, las primeras observaciones contienen valores NaN y se eliminan. La Tabla 6.1 resume la matriz resultante.")

add_table(doc,
    headers=["Concepto", "Detalle"],
    rows=[
        ["Observaciones disponibles", "271 (de 312 totales; ~41 obs. eliminadas por NaN iniciales)"],
        ["NÃºmero de features (p)", "35"],
        ["Target (y)", "gold_ret â€” retorno logarÃ­tmico mensual del oro (en puntos porcentuales)"],
        ["PerÃ­odo efectivo", "Abril 2003 â€” Octubre 2025"],
        ["Muestra de entrenamiento", "162 obs. (Abril 2003 â€” Septiembre 2016, 60%)"],
        ["Muestra de test", "109 obs. (Octubre 2016 â€” Octubre 2025, 40%)"],
    ],
    caption="Tabla 6.1 â€” EspecificaciÃ³n de la matriz de caracterÃ­sticas",
)
para(doc, "El conjunto de test (40%) cubre un perÃ­odo especialmente exigente: el ciclo de normalizaciÃ³n monetaria de 2018, la pandemia de COVID-19 y la consiguiente recuperaciÃ³n, el ciclo de subidas del tipo Fed Funds 2022-2024 (el mÃ¡s agresivo desde los aÃ±os 80), y la impresionante apreciaciÃ³n del oro en 2025 (+65% anual, 53 mÃ¡ximos histÃ³ricos).")

# 6.3
h2(doc, "6.3 MetodologÃ­a de validaciÃ³n: walk-forward")

h3(doc, "6.3.1 Fundamento")
para(doc, "La validaciÃ³n cruzada estÃ¡ndar (k-fold) baraja aleatoriamente las observaciones, lo que introduce look-ahead bias en series temporales: el modelo podrÃ­a entrenarse con datos del aÃ±o 2022 para predecir el aÃ±o 2015. Este sesgo infla artificialmente las mÃ©tricas de evaluaciÃ³n y produce estimadores inconsistentes de la capacidad predictiva fuera de muestra.")
para(doc, "La walk-forward validation con ventana expandible es el estÃ¡ndar metodolÃ³gico para la predicciÃ³n de series financieras (LÃ³pez de Prado, 2018). El esquema es el siguiente: el modelo se entrena en [1, t-1] y predice t; en el siguiente paso, el conjunto de entrenamiento se amplÃ­a a [1, t] y se predice t+1. El entrenamiento nunca incluye informaciÃ³n posterior al instante de predicciÃ³n.")
formula(doc, "Å·_t = f_t(X_1, â€¦, X_{t-1}; Î¸_t),   t = Tâ‚€ + 1, â€¦, T")
para(doc, "donde Î¸_t son los parÃ¡metros estimados en la ventana [1, t-1].")

h3(doc, "6.3.2 ParÃ¡metros de implementaciÃ³n")
para(doc, "Para acotar el coste computacional, el reentrenamiento no se realiza en cada paso sino con una frecuencia fija (refit_every). Para XGBoost se reentrena cada 3 pasos, para Random Forest cada 6, y para la LSTM tambiÃ©n cada 6 pasos (lo que genera aproximadamente 18 reentrenamientos a lo largo del perÃ­odo de test).")

h3(doc, "6.3.3 MÃ©tricas de evaluaciÃ³n")
para(doc, "Las cuatro mÃ©tricas seleccionadas cubren distintas dimensiones del error predictivo:")
formula(doc, "RMSE = âˆš(1/N Â· Î£(yâ‚œ - Å·â‚œ)Â²)")
formula(doc, "MAE = (1/N) Â· Î£|yâ‚œ - Å·â‚œ|")
formula(doc, "MAPE = (100/N) Â· Î£|( yâ‚œ - Å·â‚œ) / yâ‚œ|   (excluyendo |yâ‚œ| < 10â»â¶)")
formula(doc, "DA = (100/N) Â· Î£ ğŸ™[sign(yâ‚œ) = sign(Å·â‚œ)]")
para(doc, "La Directional Accuracy (DA) representa el porcentaje de meses en que el modelo acierta la direcciÃ³n del movimiento (subida o bajada). Un DA = 50% corresponde a una predicciÃ³n aleatoria (equivalente a lanzar una moneda), mientras que un DA superior a 50% indica capacidad informativa genuina del modelo. Para seÃ±ales de trading, el DA es la mÃ©trica mÃ¡s relevante operativamente.")

# 6.4
h2(doc, "6.4 Modelos de Ã¡rboles de decisiÃ³n")

h3(doc, "6.4.1 XGBoost")
para(doc, "XGBoost (Chen y Guestrin, 2016) es un algoritmo de gradient boosting que construye un conjunto de K Ã¡rboles de decisiÃ³n de forma secuencial: cada Ã¡rbol nuevo f_k trata de corregir los errores del Ã¡rbol anterior. La predicciÃ³n final es:")
formula(doc, "Å·â‚œ = Î£â‚– fâ‚–(Xâ‚œ)")
para(doc, "La funciÃ³n objetivo incluye un tÃ©rmino de regularizaciÃ³n sobre la estructura de los Ã¡rboles:")
formula(doc, "â„’ = Î£â‚œ â„“(yâ‚œ, Å·â‚œ) + Î£â‚– Î©(fâ‚–),   donde Î©(f) = Î³T + Â½Î»â€–wâ€–Â² + Î±â€–wâ€–â‚")
para(doc, "La configuraciÃ³n de hiperparÃ¡metros (Tabla 6.2) es conservadora: Ã¡rboles poco profundos (max_depth = 3), tasa de aprendizaje baja (0.05) y subsamplings del 80%, lo que limita el overfitting en un conjunto de entrenamiento de solo 162 observaciones.")

add_table(doc,
    headers=["Modelo", "HiperparÃ¡metro", "Valor"],
    rows=[
        ["XGBoost", "n_estimators", "300"],
        ["XGBoost", "max_depth", "3"],
        ["XGBoost", "learning_rate", "0.05"],
        ["XGBoost", "subsample", "0.8"],
        ["XGBoost", "colsample_bytree", "0.8"],
        ["XGBoost", "reg_alpha (L1)", "0.1"],
        ["XGBoost", "reg_lambda (L2)", "1.0"],
        ["Random Forest", "n_estimators", "300"],
        ["Random Forest", "max_depth", "5"],
        ["Random Forest", "min_samples_leaf", "5"],
        ["Random Forest", "max_features", "0.7"],
    ],
    caption="Tabla 6.2 â€” HiperparÃ¡metros de los modelos de Ã¡rboles",
)

h3(doc, "6.4.2 Random Forest")
para(doc, "Random Forest (Breiman, 2001) construye B Ã¡rboles de decisiÃ³n de forma paralela e independiente, cada uno sobre un subconjunto aleatorio de observaciones (bootstrap) y de variables (feature subsampling). La predicciÃ³n es el promedio de todos los Ã¡rboles:")
formula(doc, "Å·â‚œ = (1/B) Â· Î£áµ¦ Táµ¦(Xâ‚œ)")
para(doc, "A diferencia del boosting, la aleatoriedad decorrelaciona los Ã¡rboles individuales, lo que reduce la varianza del conjunto sin aumentar el sesgo. Con min_samples_leaf = 5 se impone una regularizaciÃ³n mÃ­nima especialmente importante cuando el nÃºmero de observaciones de entrenamiento es modesto.")
para(doc, "La Figura 6.2 muestra la predicciÃ³n walk-forward de los modelos de Ã¡rboles frente al precio y retorno real del oro durante el perÃ­odo de test (oct. 2016 â€” oct. 2025).")
add_figure(doc, "fig_6_02_tree_predictions.png", "Figura 6.2 â€” Predicciones walk-forward de los modelos de Ã¡rboles vs. retorno real del oro (oct. 2016 â€” oct. 2025)")

# 6.5
h2(doc, "6.5 Red neuronal recurrente: LSTM")

h3(doc, "6.5.1 Arquitectura")
para(doc, "Las redes LSTM (Long Short-Term Memory, Hochreiter y Schmidhuber, 1997) son un tipo especializado de red neuronal recurrente diseÃ±ado para capturar dependencias temporales de largo alcance. Su elemento central es la celda de memoria c_t, que puede retener o descartar informaciÃ³n de pasos anteriores mediante tres compuertas:")
para(doc, "Compuerta de olvido (forget gate):   f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)")
para(doc, "Compuerta de entrada (input gate):   i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i);   cÌƒ_t = tanh(W_c Â· [h_{t-1}, x_t] + b_c)")
para(doc, "ActualizaciÃ³n de la celda:   c_t = f_t âŠ™ c_{t-1} + i_t âŠ™ cÌƒ_t")
para(doc, "Compuerta de salida (output gate):   o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o);   h_t = o_t âŠ™ tanh(c_t)")
para(doc, "donde Ïƒ es la funciÃ³n sigmoide y âŠ™ es el producto elemento a elemento.")
para(doc, "La arquitectura implementada es deliberadamente parsimoniosa, apropiada para series macroeconÃ³micas con 271 observaciones: GoldLSTM consiste en una capa LSTM con 32 unidades seguida de una capa lineal Linear(32, 1). El modelo recibe como entrada una secuencia de seq_len = 6 meses de las 35 variables de features (estandarizadas) y produce el retorno del oro predicho para el mes siguiente.")

h3(doc, "6.5.2 Entrenamiento y early stopping")
para(doc, "Los hiperparÃ¡metros de entrenamiento se recogen en la Tabla 6.4:")

add_table(doc,
    headers=["HiperparÃ¡metro", "Valor", "DescripciÃ³n"],
    rows=[
        ["hidden_size", "32", "Unidades en la capa LSTM"],
        ["num_layers", "1", "NÃºmero de capas LSTM apiladas"],
        ["seq_len", "6", "Longitud de la ventana de entrada (meses)"],
        ["batch_size", "16", "TamaÃ±o del mini-batch"],
        ["lr", "0.001", "Tasa de aprendizaje (Adam)"],
        ["max_epochs", "150", "Ã‰pocas mÃ¡ximas de entrenamiento"],
        ["patience", "20", "Paciencia del early stopping"],
        ["val_frac", "0.15", "FracciÃ³n de train reservada para validaciÃ³n interna"],
    ],
    caption="Tabla 6.4 â€” HiperparÃ¡metros de la red LSTM",
)
para(doc, "Para evitar el sobreajuste durante el entrenamiento se implementa early stopping: las Ãºltimas val_frac = 15% de las observaciones de entrenamiento (en orden temporal) se reservan como validaciÃ³n interna. Si la pÃ©rdida de validaciÃ³n no mejora durante 20 Ã©pocas consecutivas, el entrenamiento se interrumpe y se recuperan los pesos correspondientes a la mÃ­nima pÃ©rdida de validaciÃ³n observada.")
para(doc, "En el walk-forward, el escalador StandardScaler se reajusta en cada paso sobre el conjunto de entrenamiento disponible hasta t-1, previniendo que los estadÃ­sticos de escala incorporen informaciÃ³n del perÃ­odo de test.")
add_figure(doc, "fig_6_05_lstm_predictions.png", "Figura 6.5 â€” Predicciones walk-forward de la LSTM vs. retorno real del oro (oct. 2016 â€” oct. 2025)")

# 6.6
h2(doc, "6.6 Interpretabilidad: anÃ¡lisis SHAP")

h3(doc, "6.6.1 Fundamento teÃ³rico de los valores de Shapley")
para(doc, "Los valores de Shapley (Shapley, 1953) son la Ãºnica soluciÃ³n axiomÃ¡ticamente justa al problema de atribuciÃ³n del valor en teorÃ­a de juegos cooperativos. En el contexto del machine learning (Lundberg y Lee, 2017), el valor SHAP de la caracterÃ­stica j para la predicciÃ³n i se define como:")
formula(doc, "Ï†â±¼(i) = Î£_{S âŠ† F\\{j}}  |S|!(|F|-|S|-1)!/|F|! Â· [f(S âˆª {j}) â€“ f(S)]")
para(doc, "El valor Ï†â±¼(i) puede interpretarse como la contribuciÃ³n marginal promedio de la caracterÃ­stica j a la predicciÃ³n i, promediada sobre todas las posibles coaliciones de variables. Para los modelos de Ã¡rboles, el algoritmo TreeSHAP (Lundberg et al., 2020) calcula los valores de Shapley exactos en tiempo polinomial O(TLDÂ²), donde T es el nÃºmero de Ã¡rboles, L el nÃºmero de hojas y D la profundidad mÃ¡xima.")

h3(doc, "6.6.2 Importancia global de las variables")
para(doc, "La Figura 6.1 muestra la importancia media de cada variable, medida como el valor absoluto promedio de los valores SHAP sobre el conjunto de test:")
formula(doc, "Ï†Ì„â±¼ = (1/N) Â· Î£áµ¢ |Ï†â±¼(i)|")
add_figure(doc, "fig_6_01_shap_importance.png", "Figura 6.1 â€” Importancia global de variables por valor SHAP medio (XGBoost, muestra de test)")

add_table(doc,
    headers=["PosiciÃ³n", "Variable", "SHAP medio |Ï†|", "InterpretaciÃ³n"],
    rows=[
        ["1", "CPI YoY (t-1)", "0.954", "La inflaciÃ³n realizada del mes anterior es el predictor mÃ¡s potente del retorno del oro. Consecuente con el rol del oro como cobertura inflacionaria."],
        ["2", "TIPS 10Y (t-2)", "0.617", "Los tipos de interÃ©s reales con dos meses de retardo muestran fuerte seÃ±al. Consistente con la causalidad Granger del Cap. 5: TIPS causa al oro."],
        ["3", "Ret. oro (t-1)", "0.526", "El retorno pasado del propio oro (momentum de 1 mes) tiene un poder predictivo significativo, aunque con signo variable segÃºn el entorno."],
        ["4", "Breakeven (t-3)", "0.485", "Las expectativas de inflaciÃ³n a 3 meses vista anticipan la demanda futura de oro como activo real."],
        ["5", "WTI (t-2)", "0.423", "El retorno del petrÃ³leo con 2 meses de retardo actÃºa como proxy de presiones inflacionarias globales."],
        ["6", "S&P 500 (t-1)", "0.397", "La renta variable rezagada captura el efecto sustituciÃ³n: en perÃ­odos de apreciaciÃ³n bursÃ¡til, el oro pierde atractivo relativo."],
        ["7", "Vol3 oro", "0.379", "La volatilidad realizada reciente del oro influye en las predicciones, reflejando la asimetrÃ­a documentada en el Cap. 5."],
        ["8", "DXY (t-3)", "0.329", "El dÃ³lar con 3 meses de retardo captura la inercia del ciclo del dÃ³lar, que afecta inversamente al oro."],
    ],
    caption="Tabla 6.3 â€” Top 8 variables por importancia SHAP (XGBoost)",
)
para(doc, "Hallazgo principal: los tres bloques mÃ¡s informativos son (1) inflaciÃ³n y expectativas inflacionarias, (2) tipos de interÃ©s reales, y (3) momentum del propio oro. Esto es coherente con la hipÃ³tesis central del TFG y con los resultados economÃ©tricos del CapÃ­tulo 5.")
para(doc, "Hallazgo destacable: la variable is_crisis (episodio de crisis) ocupa el Ãºltimo lugar en importancia (Ï†Ì„ = 0.029), lo que sugiere que los modelos de Ã¡rboles ya capturan indirectamente el rÃ©gimen de mercado a travÃ©s de las variables continuas (VIX, retornos extremos, etc.), sin necesitar una dummy explÃ­cita.")

h3(doc, "6.6.3 Summary plot (SHAP beeswarm)")
para(doc, "La Figura 6.3 muestra el summary plot de SHAP: cada punto representa una observaciÃ³n del test, el eje horizontal es el valor SHAP (impacto positivo hacia la derecha, negativo hacia la izquierda), y el color indica si el valor de la feature es alto (rojo) o bajo (azul).")
add_figure(doc, "fig_6_03_shap_summary.png", "Figura 6.3 â€” SHAP summary plot (beeswarm): direcciÃ³n e intensidad de la contribuciÃ³n de cada variable")
para(doc, "Este grÃ¡fico revela la direcciÃ³n de cada relaciÃ³n: CPI YoY alto â†’ SHAP positivo â†’ predicciÃ³n de retorno del oro mÃ¡s alto (oro como cobertura inflacionaria). TIPS 10Y alto â†’ SHAP negativo â†’ tipos reales elevados aumentan el coste de oportunidad de mantener oro. S&P 500 alto â†’ SHAP negativo â†’ relaciÃ³n de sustituciÃ³n entre renta variable y oro.")

h3(doc, "6.6.4 AnÃ¡lisis de episodios clave (SHAP waterfall)")
para(doc, "La Figura 6.4 descompone la predicciÃ³n del modelo para tres meses representativos: la crisis financiera global (GFC, octubre 2008), el crash de COVID (marzo 2020), y la confluencia de catalizadores de diciembre 2025.")
add_figure(doc, "fig_6_04_shap_waterfall.png", "Figura 6.4 â€” SHAP waterfall: descomposiciÃ³n de predicciones en episodios clave (GFC oct. 2008, COVID mar. 2020, dic. 2025)")
para(doc, "El anÃ¡lisis waterfall evidencia que el mecanismo de transmisiÃ³n es estable pero el peso relativo de cada variable cambia entre episodios: en el GFC (2008), los tipos reales y la inflaciÃ³n dominan la seÃ±al alcista del oro; en el COVID (2020), el VIX y el momentum reciente del oro amplifican la predicciÃ³n positiva; en diciembre 2025, la confluencia de inflaciÃ³n persistente, tipos reales negativos en tÃ©rminos reales y compras rÃ©cord de bancos centrales genera la seÃ±al SHAP mÃ¡s intensa del perÃ­odo.")

# 6.7
h2(doc, "6.7 Comparativa de modelos")

h3(doc, "6.7.1 Resultados numÃ©ricos")
para(doc, "La Tabla 6.5 presenta las cuatro mÃ©tricas de evaluaciÃ³n walk-forward para todos los modelos sobre el perÃ­odo de test (octubre 2016 â€” octubre 2025, N = 109 observaciones).")

add_table(doc,
    headers=["Modelo", "RMSE (pp)", "MAE (pp)", "MAPE (%)", "DA (%)", "N"],
    rows=[
        ["Naive (random walk)", "5.054", "4.043", "244.9", "55.9", "109"],
        ["XGBoost", "4.340", "3.476", "308.0", "52.3", "109"],
        ["Random Forest", "3.882", "3.181", "226.5", "58.7", "109"],
        ["LSTM (mejor modelo)", "3.815", "3.142", "278.8", "61.5", "109"],
    ],
    caption="Tabla 6.5 â€” Comparativa de modelos predictivos (walk-forward, test: oct. 2016 â€” oct. 2025)",
)
para(doc, "Nota: RMSE y MAE en puntos porcentuales (pp) del retorno logarÃ­tmico mensual. La desviaciÃ³n tÃ­pica incondicional de gold_ret en la muestra completa es 4.65 pp.")
add_figure(doc, "fig_6_06_model_comparison.png", "Figura 6.6 â€” Comparativa visual de modelos: RMSE, MAE y Directional Accuracy (walk-forward, test N=109)")

h3(doc, "6.7.2 DiscusiÃ³n de resultados")
para(doc, "La LSTM obtiene el mejor rendimiento en todas las mÃ©tricas clave, con RMSE = 3.815 pp (el 82.1% de la desviaciÃ³n tÃ­pica incondicional) y DA = 61.5%. Esto indica que la red recurrente captura dependencias temporales que los modelos de Ã¡rboles, que no tienen memoria secuencial explÃ­cita, no explotan completamente.")
para(doc, "El Random Forest supera a XGBoost en todas las mÃ©tricas, a pesar de su arquitectura mÃ¡s simple. Este resultado es frecuente en series financieras cortas (n < 500): el bagging del RF es mÃ¡s robusto ante la sobreparametrizaciÃ³n que el boosting secuencial, que puede seguir minimizando el error de entrenamiento en ausencia de suficientes observaciones.")
para(doc, "El resultado mÃ¡s llamativo es que XGBoost (DA = 52.3%) queda por debajo del benchmark naive (DA = 55.9%), aunque supera al naive en RMSE y MAE. Este patrÃ³n sugiere que XGBoost introduce ruido en la direcciÃ³n del movimiento: mejora la magnitud de las predicciones cerca de cero, pero falla en los meses de retorno extremo (que son los que importan para las decisiones de inversiÃ³n). El RF y la LSTM son mÃ¡s robustos a este problema.")
para(doc, "Sobre el MAPE elevado: los valores de MAPE (226-308%) son caracterÃ­sticos de la predicciÃ³n de retornos financieros. El MAPE es sensible a retornos cercanos a cero: si el retorno real es +0.1% y el modelo predice -0.2%, el error porcentual es del 300%, aunque el error absoluto sea insignificante. Por ello, el MAPE debe leerse en combinaciÃ³n con RMSE y DA.")
para(doc, "ComparaciÃ³n con el benchmark economÃ©trico: el VECM del CapÃ­tulo 5 genera predicciones fuera de muestra con una DA tÃ­pica del 54-56% en series de activos financieros (Stock y Watson, 2001). La LSTM (DA = 61.5%) y el RF (DA = 58.7%) mejoran materialmente esta referencia, justificando el enfoque hÃ­brido economÃ©trico-ML de este TFG.")

h3(doc, "6.7.3 Limitaciones")
para(doc, "Se seÃ±alan tres limitaciones relevantes:")
para(doc, "1. TamaÃ±o de la muestra: con 271 observaciones efectivas y 35 features, el cociente parÃ¡metros/observaciones es relativamente alto, lo que puede inflar las mÃ©tricas favorables y aumentar la varianza de los estimadores.")
para(doc, "2. Coste de transacciÃ³n: las mÃ©tricas de evaluaciÃ³n no consideran costes de transacciÃ³n (spreads, comisiones, deslizamiento). Un sistema de trading basado en seÃ±ales con DA â‰ˆ 61% podrÃ­a perder rentabilidad neta tras fricciÃ³n.")
para(doc, "3. Estabilidad de la DA: la Directional Accuracy puede variar significativamente segÃºn el subperÃ­odo de test. Una evaluaciÃ³n mÃ¡s robusta requerirÃ­a un anÃ¡lisis de la DA por ventanas deslizantes o por episodio de mercado.")

# 6.8
h2(doc, "6.8 Conclusiones del capÃ­tulo")
para(doc, "Este capÃ­tulo ha implementado tres algoritmos de machine learning para predecir el retorno mensual del oro sobre el perÃ­odo de test octubre 2016 â€” octubre 2025. Las principales conclusiones son:")
para(doc, "1. La LSTM es el modelo mÃ¡s preciso (RMSE = 3.815 pp, DA = 61.5%), aprovechando la estructura secuencial de los datos macroeconÃ³micos para capturar dependencias temporales de hasta 6 meses.")
para(doc, "2. El Random Forest es el modelo de mayor robustez entre los de Ã¡rboles (RMSE = 3.882 pp, DA = 58.7%), superando al XGBoost en todas las mÃ©tricas pese a su arquitectura mÃ¡s sencilla.")
para(doc, "3. El anÃ¡lisis SHAP revela que los predictores mÃ¡s influyentes son la inflaciÃ³n realizada (CPI YoY), los tipos de interÃ©s reales (TIPS 10Y) y el momentum del propio oro, confirmando los hallazgos economÃ©tricos del CapÃ­tulo 5 con una metodologÃ­a completamente diferente e independiente.")
para(doc, "4. Todos los modelos de ML superan al benchmark naive en error absoluto (RMSE, MAE), validando que los patrones macroeconÃ³micos tienen contenido informativo sobre la direcciÃ³n del precio del oro a un horizonte de un mes.")
para(doc, "5. El anÃ¡lisis integrado de los CapÃ­tulos 5 y 6 confluye en una conclusiÃ³n robusta: los tipos de interÃ©s reales y las expectativas inflacionarias son los determinantes de mayor peso del precio del oro, tanto en el plano estructural (VECM, Granger) como en el predictivo (SHAP values). La asimetrÃ­a de la volatilidad (GJR-GARCH) y el efecto de los episodios de crisis refuerzan la narrativa del oro como activo refugio y cobertura inflacionaria.")

# Save
output_path = "Capitulo 06 ML.docx"
doc.save(output_path)
print(f"Documento guardado: {output_path}")
print(f"  PÃ¡rrafos: {len(doc.paragraphs)}")
print(f"  Tablas:   {len(doc.tables)}")
